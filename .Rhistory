score2 <- (scores2$RawScore - 12) / 4
hist(scores2$RawScore - 12, breaks = "FD", probability = TRUE)
hist(score2, breaks = "FD", probability = TRUE)
mu <- mean(score2) ; mu
var <- var(score2); var
hist(score2, breaks = "FD", probability = TRUE)
curve(dgamma(x, mu), add = TRUE, col = "red")
score2 <- (scores2$RawScore - min(scores2$RawScore)) / 4
mu <- mean(score2) ; mu
var <- var(score2); var
hist(score2, breaks = "FD", probability = TRUE)
#a gamma distribution appears to fit quite well once we've transformed the data
curve(dgamma(x, mu), add = TRUE, col = "red")
library(ggplot2)
library(ggplot2)
#messy but there are two clear peaks we can isolate
hist(compas$RawScore, breaks = "FD", probability = TRUE)
#first chunk -> scores < 7 (this is an estimate,
#should we try using a chunking algorithm like k-means or something?)
scores <- compas[which(compas$RawScore < 7),]; head(scores)
max <- max(scores$RawScore)
min <- min(scores$RawScore)
#squeezing the values to be between 0 and 1
scores$score <- (max - scores$RawScore) / (max - min)
ggplot(scores, aes(x=score)) + geom_histogram()
ggplot(compas, aes(x=RawScore)) + geom_histogram()
#first chunk -> scores < 7 (this is an estimate,
#should we try using a chunking algorithm like k-means or something?)
scores <- compas[which(compas$RawScore < 7),]; head(scores)
max <- max(scores$RawScore)
min <- min(scores$RawScore)
#squeezing the values to be between 0 and 1
scores$score <- (max - scores$RawScore) / (max - min)
ggplot(scores, aes(x=score)) + geom_histogram()
curve(dbeta(x, param1, param2), add = TRUE, col = "red")
plot <- ggplot(scores, aes(x=score)) + geom_histogram(); plot
compas <- read.csv ("compass/compas-scores-raw.csv"); head(compas)
library(ggplot2)
#messy but there are two clear peaks we can isolate
ggplot(compas, aes(x=RawScore)) + geom_histogram()
#first chunk -> scores < 7 (this is an estimate,
#should we try using a chunking algorithm like k-means or something?)
scores <- compas[which(compas$RawScore < 7),]; head(scores)
max <- max(scores$RawScore)
min <- min(scores$RawScore)
#squeezing the values to be between 0 and 1
scores$score <- (max - scores$RawScore) / (max - min)
plot <- ggplot(scores, aes(x=score)) + geom_histogram(); plot
mu <- mean(score); mu
var <- var(score); var
#used Wolfram Alpha to find parameters for a beta distribution by solving a system of equations
param1 <- 15162/3725
param2 <- 11913/3725
stat <- stat_function(aes(x = x, y = y), fun = dbeta, colour="red", n = 100,
args = list(shape1 = param1, shape2 = param2))
plot + stat
stat <- stat_function(aes(x = x), fun = dbeta, colour="red", n = 100,
args = list(shape1 = param1, shape2 = param2))
plot + stat
ggplot(scores, aes(x=score)) + geom_histogram() + stat_function(aes(x = x), fun = dbeta, colour="red", n = 100,
args = list(shape1 = param1, shape2 = param2))
stat <- stat_function(fun = dbeta, colour="red", n = 100,
args = list(shape1 = param1, shape2 = param2))
plot + stat
stat
compas <- read.csv ("compass/compas-scores-raw.csv"); head(compas)
stat + ggplot()
stat <- plot_beta_dist_gg(alpha = param1, beta = param2)
plot + stat
plot <- ggplot(scores, aes(x=score)) +  geom_density(); plot
plot <- ggplot(scores, aes(x=score)) +  geom_historgram () + geom_density(); plot
plot <- ggplot(scores, aes(x=score)) +  geom_histogram () + geom_density(); plot
plot <- ggplot(scores, aes(x=score, y = ..density..)) +  geom_density(); plot
plot <- ggplot(scores) + geom_histogram(aes(x=score, y = ..density..)); plot
plot <- ggplot(scores) + geom_histogram(aes(x=score, y = ..density..), binwidth = 1); plot
plot <- ggplot(scores) + geom_histogram(aes(x=score, y = ..density..), binwidth = 0.01); plot
plot <- ggplot(scores) + geom_histogram(aes(x=score, y = ..density..), binwidth = 0.25); plot
plot <- ggplot(scores) + geom_histogram(aes(x=score, y = ..density..), binwidth = 0.05); plot
stat <- plot_beta_dist_gg(alpha = param1, beta = param2)
stat <- plot_beta_dist_gg(alpha = param1, beta = param2)
plot <- ggplot(scores, aes(score)) + geom_histogram(aes(y = stat(density))); plot
stat <- stat_function(fun = dbeta, args = list(param1, param2), lwd = 2, col = "red")
plot + stat
#second chunk of scores -> scores > 7
scores2 <- compas[which(compas$RawScore >= 7),]
#translating the data to the origin
#and transforming it to lie between 0 and 10
scores2$score <- (scores2$RawScore - min(scores2$RawScore)) / 4
mu <- mean(scores2$score) ; mu
var <- var(scores2$score); var
plot <- ggplot(scores2, aes(score)) + geom_histogram(aes(y = stat(density))); plot
plot <- ggplot(scores2, aes(score)) + geom_histogram(aes(y = stat(density)), binwidth = 0.05); plot
plot <- ggplot(scores2, aes(score)) + geom_histogram(aes(y = stat(density)), binwidth = 0.5); plot
plot <- ggplot(scores2, aes(score)) + geom_histogram(aes(y = stat(density)), binwidth = 0.25); plot
#second chunk of scores -> scores > 7
scores2 <- compas[which(compas$RawScore >= 7),]
#translating the data to the origin
#and transforming it to lie between 0 and 10
scores2$score <- (scores2$RawScore - min(scores2$RawScore)) / 4
mu <- mean(scores2$score) ; mu
var <- var(scores2$score); var
plot <- ggplot(scores2, aes(score)) + geom_histogram(aes(y = stat(density)), binwidth = 0.25); plot
#let's try dgamma
stat <- stat_function(fun = dgamma, args = list(mu), lwd = 1, col = "red")
plot + stat
compas <- read.csv ("compass/compas-scores-raw.csv"); head(compas)
library(ggplot2)
#messy but there are two clear peaks we can isolate
ggplot(compas, aes(x=RawScore)) + geom_histogram()
#first cluster -> scores < 7 (this is an estimate,
#should we try using a clustering algorithm like k-means or something?)
scores <- compas[which(compas$RawScore < 7),]; head(scores)
max <- max(scores$RawScore)
min <- min(scores$RawScore)
#squeezing the values to be between 0 and 1
scores$score <- (max - scores$RawScore) / (max - min)
plot <- ggplot(scores, aes(score)) + geom_histogram(aes(y = stat(density))); plot
mu <- mean(scores$score); mu
var <- var(scores$score); var
#used Wolfram Alpha to find parameters for a beta distribution by solving a system of equations
param1 <- 15162/3725
param2 <- 11913/3725
#let's try dbeta
stat <- stat_function(fun = dbeta, args = list(param1, param2), lwd = 2, col = "red")
#that looks quite accurate!
plot + stat
stat <- stat_function(fun = dbeta, args = list(param1, param2), lwd = 1, col = "red")
#that looks quite accurate!
plot + stat
#second cluster -> scores > 7
scores2 <- compas[which(compas$RawScore >= 7),]
#translating the data to the origin
#and transforming it to lie between 0 and 10
scores2$score <- (scores2$RawScore - min(scores2$RawScore)) / 4
mu <- mean(scores2$score) ; mu
var <- var(scores2$score); var
plot <- ggplot(scores2, aes(score)) + geom_histogram(aes(y = stat(density)), binwidth = 0.25); plot
#let's try dgamma
stat <- stat_function(fun = dgamma, args = list(mu), lwd = 1, col = "red")
#dgamma appears to fit!
plot + stat
#do our transformations make the distributions that we fit useless?
clusters <- kmeans (compas, 2)
compas <- read.csv ("compass/compas-scores-raw.csv"); head(compas)
library(ggplot2)
clusters <- kmeans (compas, 2)
clusters <- kmeans(compas$RawScore, 2)
clusters <- kmeans(compas$RawScore, 2); clusters
compas$clusters <- clusters
compas$clusters <- clusters$vector
clusters <- kmeans(compas$RawScore, 2)$vector; clusters
clusters <- kmeans(compas$RawScore, 2)$ClusteringVector; clusters
clusters <- kmeans(compas$RawScore, 2)$cluster; clusters
compas$clusters <- clusters
scores <- compas[which(compas$clusters = 1)]
scores <- compas[which(compas$clusters == 1),]
plot <- ggplot(scores, aes(score)) + geom_histogram(aes(y = stat(density))); plot
scores$score <- (max - scores$RawScore) / (max - min)
max <- max(scores$RawScore)
min <- min(scores$RawScore)
scores$score <- (max - scores$RawScore) / (max - min)
plot <- ggplot(scores, aes(score)) + geom_histogram(aes(y = stat(density))); plot
compas <- read.csv ("compass/compas-scores-raw.csv"); head(compas)
library(ggplot2)
#messy but there are two clear clusters we can isolate
ggplot(compas, aes(x=RawScore)) + geom_histogram()
#first cluster -> scores < 7 (this is an estimate,
#should we try using a clustering algorithm like k-means or something?)
scores <- compas[which(compas$RawScore < 7),]; head(scores)
max <- max(scores$RawScore)
min <- min(scores$RawScore)
#squeezing the values to be between 0 and 1
scores$score <- (max - scores$RawScore) / (max - min)
plot <- ggplot(scores, aes(score)) + geom_histogram(aes(y = stat(density))); plot
mu <- mean(scores$score); mu
var <- var(scores$score); var
#used Wolfram Alpha to find parameters for a beta distribution by solving a system of equations
param1 <- 15162/3725
param2 <- 11913/3725
#let's try dbeta
stat <- stat_function(fun = dbeta, args = list(param1, param2), lwd = 1, col = "red")
#that looks quite accurate!
plot + stat
#second cluster -> scores > 7
scores2 <- compas[which(compas$RawScore >= 7),]
#translating the data to the origin
#and transforming it to lie between 0 and 10
scores2$score <- (scores2$RawScore - min(scores2$RawScore)) / 4
mu <- mean(scores2$score) ; mu
var <- var(scores2$score); var
plot <- ggplot(scores2, aes(score)) + geom_histogram(aes(y = stat(density)), binwidth = 0.25); plot
#let's try dgamma
stat <- stat_function(fun = dgamma, args = list(mu), lwd = 1, col = "red")
#dgamma appears to fit!
plot + stat
#do our transformations make the distributions that we fit useless?
#View(scores)
scores_black <- compas[which(compas$Ethnic_Code_Text == 'African-American'),]
#View(scores_black)
mean(scores_black$RawScore)
compas <- read.csv ("compass/compas-scores-raw.csv"); head(compas)
library(ggplot2)
#messy but there are two clear clusters we can isolate
ggplot(compas, aes(x=RawScore)) + geom_histogram()
#first cluster -> scores < 7 (this is an estimate,
#should we try using a clustering algorithm like k-means or something?)
scores <- compas[which(compas$RawScore < 7),]; head(scores)
max <- max(scores$RawScore)
min <- min(scores$RawScore)
#squeezing the values to be between 0 and 1
scores$score <- (max - scores$RawScore) / (max - min)
plot <- ggplot(scores, aes(score)) + geom_histogram(aes(y = stat(density))); plot
mu <- mean(scores$score); mu
var <- var(scores$score); var
#used Wolfram Alpha to find parameters for a beta distribution by solving a system of equations
param1 <- 15162/3725
param2 <- 11913/3725
#let's try dbeta
stat <- stat_function(fun = dbeta, args = list(param1, param2), lwd = 1, col = "red")
#that looks quite accurate!
plot + stat
#second cluster -> scores > 7
scores2 <- compas[which(compas$RawScore >= 7),]
#translating the data to the origin
#and transforming it to lie between 0 and 10
scores2$score <- (scores2$RawScore - min(scores2$RawScore)) / 4
mu <- mean(scores2$score) ; mu
var <- var(scores2$score); var
plot <- ggplot(scores2, aes(score)) + geom_histogram(aes(y = stat(density)), binwidth = 0.25); plot
#let's try dgamma
stat <- stat_function(fun = dgamma, args = list(mu), lwd = 1, col = "red")
#dgamma appears to fit!
plot + stat
#do our transformations make the distributions that we fit useless?
#View(scores)
scores_black <- compas[which(compas$Ethnic_Code_Text == 'African-American'),]
#View(scores_black)
mean(scores_black$RawScore)
scores_white <- compas[which(compas$Ethnic_Code_Text == 'Caucasian'),]
mean(scores_white$RawScore)
diff <- mean(scores_black$RawScore) - mean(scores_white$RawScore); diff
n0 = length(scores_white$RawScore)
n1 = length(scores_black$RawScore)
all_scores <- c(scores_white$RawScore, scores_black$RawScore)
diffs <- vector()
for (i in 1:5000){
sampled_white <- sample(all_scores, n0, replace = FALSE)
sampled_black <- all_scores[! all_scores %in% sampled_white]
diffs <- c(diffs, mean(sampled_black) - mean(sampled_white))
}
hist(diffs, color = 'red')
abline(v = diff)
hist(diffs, color = 'red')
abline(v = diff)
percentile <- ecdf(diffs)
(1 - percentile(0.5879))*2
hist(scores_white$RawScore)
hist(scores_black$RawScore)
scores_white$id <- 'white'
scores_black$id <- 'black'
#df <- data.frame(white = scores_white$RawScore, black = scores_black$RawScore)
Lengths <- data.frame(rbind(scores_black, scores_white))
ggplot(Lengths, aes(RawScore, fill = id)) +
geom_histogram(alpha = 0.5, aes(y = ..density..), position = 'identity')
compas <- read.csv ("compass/compas-scores-raw.csv"); head(compas)
library(ggplot2)
chisq.test(compas$DisplayText, compas$Ethnic_Code_Text)
CrossTable(compas$Ethnic_Code_Text, compas$DisplayText)
library(gmodels)
CrossTable(compas$Ethnic_Code_Text, compas$DisplayText)
CrossTable(compas$Ethnic_Code_Text, compas$DisplayText,  prop.t=FALSE, prop.r=FALSE, prop.c=FALSE)
compass <- read.csv ("compass/compas-scores-raw.csv"); head(compas)
#REQUIRED: dataframe
compas <- data.frame(compass)
library(ggplot2)
library(gmodels)
compass <- read.csv ("compass/compas-scores-raw.csv"); head(compass)
#REQUIRED: dataframe
compas <- data.frame(compass); head(compas)
library(ggplot2)
library(gmodels)
ggplot(compas, aes(x=RawScore)) + geom_histogram()
scores <- compas[which(compas$RawScore < 7),]; head(scores)
max <- max(scores$RawScore)
min <- min(scores$RawScore)
#squeezing the values to be between 0 and 1
scores$score <- (max - scores$RawScore) / (max - min)
#histogram using ggplot
plot <- ggplot(scores, aes(score)) + geom_histogram(aes(y = stat(density))); plot
mu <- mean(scores$score); mu
var <- var(scores$score); var
#used Wolfram Alpha to find parameters for a beta distribution by solving a system of equations
param1 <- 15162/3725
param2 <- 11913/3725
#let's try dbeta
stat <- stat_function(fun = dbeta, args = list(param1, param2), lwd = 1, col = "red")
#that looks quite accurate!
plot + stat
#second cluster -> scores > 7
scores2 <- compas[which(compas$RawScore >= 7),]
#translating the data to the origin
#and transforming it to lie between 0 and 10
scores2$score <- (scores2$RawScore - min(scores2$RawScore)) / 4
mu <- mean(scores2$score) ; mu
var <- var(scores2$score); var
plot <- ggplot(scores2, aes(score)) + geom_histogram(aes(y = stat(density)), binwidth = 0.25); plot
#let's try dgamma
stat <- stat_function(fun = dgamma, args = list(mu), lwd = 1, col = "red")
#dgamma appears to fit!
plot + stat
#do our transformations make the distributions that we fit useless?
#View(scores)
scores_black <- compas[which(compas$Ethnic_Code_Text == 'African-American'),]
#View(scores_black)
mean(scores_black$RawScore)
scores_white <- compas[which(compas$Ethnic_Code_Text == 'Caucasian'),]
mean(scores_white$RawScore)
diff <- mean(scores_black$RawScore) - mean(scores_white$RawScore); diff
n0 = length(scores_white$RawScore)
n1 = length(scores_black$RawScore)
all_scores <- c(scores_white$RawScore, scores_black$RawScore)
diffs <- vector()
for (i in 1:5000){
sampled_white <- sample(all_scores, n0, replace = FALSE)
sampled_black <- all_scores[! all_scores %in% sampled_white]
diffs <- c(diffs, mean(sampled_black) - mean(sampled_white))
}
hist(diffs, color = 'red')
abline(v = diff)
percentile <- ecdf(diffs)
(1 - percentile(0.5879))*2
hist(scores_white$RawScore)
hist(scores_black$RawScore)
scores_white$id <- 'white'
scores_black$id <- 'black'
#df <- data.frame(white = scores_white$RawScore, black = scores_black$RawScore)
Lengths <- data.frame(rbind(scores_black, scores_white))
ggplot(Lengths, aes(RawScore, fill = id)) +
geom_histogram(alpha = 0.5, aes(y = ..density..), position = 'identity')
#are race and a display text of "Risk of Violence" independent?
#extracting logical columns
risk_Log <- compas$DisplayText == "Risk of Violence"; sum(risk_Log)
race_Log <- compas$Ethnic_Code_Text != "Caucasian"; sum(race_Log)
#REQUIRED: two categorical columns
dataLog <- data.frame(risk_Log, race_Log)
#a contingency table showing all 4 options
Obs <- table (dataLog$risk_Log, dataLog$race_Log); Obs
#what we would expect if the factors are independent
Expected <- outer(rowSums(Obs), colSums(Obs))/sum(Obs); Expected
#chi-sq: p value is 1 meaning that there is 100% chance that race and risk of violence
#as display text are independent
chisq.test(dataLog$risk_Log, dataLog$race_Log)
#Paul's method of calculating chi-sq value
ChiSq <-function(Obs,Exp){
sum((Obs-Exp)^2/Exp)
}
#same chi-sq statistic and p value as above
CSq <- ChiSq(Obs, Expected); CSq
pchisq(CSq, df = 3, lower.tail = FALSE)
#barplot of various display texts
#they are all perfectly equal! weird!
plot <- ggplot(data = compas, aes(x = factor(DisplayText))) +
geom_bar(stat="count", width=0.7, fill="steelblue") + theme_minimal() +
ggtitle("Display Text vs Frequency") + xlab("Display Text") + ylab("Frequency"); plot
head(dataLog$raceLog)
library(ggplot2)
library(gmodels)
compass <- read.csv ("compass/compas-scores-raw.csv"); head(compass)
#REQUIRED: dataframe
compas <- data.frame(compass); head(compas)
risk_Log <- compas$DisplayText == "Risk of Violence"; sum(risk_Log)
race_Log <- compas$Ethnic_Code_Text != "Caucasian"; sum(race_Log)
sum(dataLog$raceLog*dataLog$riskLog)/length(dataLog$raceLog)
#are race and a display text of "Risk of Violence" independent?
#extracting logical columns
risk_Log <- compas$DisplayText == "Risk of Violence"; sum(risk_Log)
race_Log <- compas$Ethnic_Code_Text != "Caucasian"; sum(race_Log)
#REQUIRED: two categorical columns
dataLog <- data.frame(risk_Log, race_Log)
#a contingency table showing all 4 options
Obs <- table (dataLog$risk_Log, dataLog$race_Log); Obs
#what we would expect if the factors are independent
Expected <- outer(rowSums(Obs), colSums(Obs))/sum(Obs); Expected
#REQUIRED: analysis of contingency tables
#WOAH these are the exact same
#chi-sq: p value is 1 meaning that there is 100% chance that race and risk of violence
#as display text are independent
chisq.test(dataLog$risk_Log, dataLog$race_Log)
#Paul's method of calculating chi-sq value
ChiSq <-function(Obs,Exp){
sum((Obs-Exp)^2/Exp)
}
#same chi-sq statistic and p value as above
CSq <- ChiSq(Obs, Expected); CSq
pchisq(CSq, df = 3, lower.tail = FALSE)
#REQUIRED: p value using a distribution function
#POINT 9: relationship that we expected to be statistically significant but was not
#are race and a display text of "Risk of Recidivism" independent?
#extracting logical columns
recid_Log <- compas$DisplayText == "Risk of Recidivism"; sum(recid_Log)
dataLog$recid_Log <- recid_Log
#a contingency table showing all 4 options
Obs <- table (dataLog$recid_Log, dataLog$race_Log); Obs
#what we would expect if the factors are independent
Expected <- outer(rowSums(Obs), colSums(Obs))/sum(Obs); Expected
#WOAH these are the exact same
#chi-sq: p value is 1 meaning that there is 100% chance that race and risk of violence
#as display text are independent
chisq.test(dataLog$recid_Log, dataLog$race_Log)
#Paul's method of calculating chi-sq value
ChiSq <-function(Obs,Exp){
sum((Obs-Exp)^2/Exp)
}
#same chi-sq statistic and p value as above
CSq <- ChiSq(Obs, Expected); CSq
pchisq(CSq, df = 3, lower.tail = FALSE)
#are race and a display text of "Risk of Failure to Appear" independent?
#extracting logical columns
appear_Log <- compas$DisplayText == "Risk of Recidivism"; sum(appear_Log)
dataLog$appear_Log <- appear_Log
#a contingency table showing all 4 options
Obs <- table (dataLog$appear_Log, dataLog$race_Log); Obs
#what we would expect if the factors are independent
Expected <- outer(rowSums(Obs), colSums(Obs))/sum(Obs); Expected
#WOAH these are the exact same
#chi-sq: p value is 1 meaning that there is 100% chance that race and risk of violence
#as display text are independent
chisq.test(dataLog$appear_Log, dataLog$race_Log)
#Paul's method of calculating chi-sq value
ChiSq <-function(Obs,Exp){
sum((Obs-Exp)^2/Exp)
}
#same chi-sq statistic and p value as above
CSq <- ChiSq(Obs, Expected); CSq
pchisq(CSq, df = 3, lower.tail = FALSE)
#our three contingency tables displayed nicely
#Risk of Violence versus Race (Caucasian or not)
CrossTable(dataLog$risk_Log, dataLog$race_Log, dnn= c("Caucasian", "Risk of Violence"), prop.t=FALSE, prop.r=FALSE, prop.c=FALSE, prop.chisq = FALSE)
#Risk of Recidivism versus Race (Caucasian or not)
CrossTable(dataLog$recid_Log, dataLog$race_Log, dnn= c("Caucasian", "Risk of Recidivism"), prop.t=FALSE, prop.r=FALSE, prop.c=FALSE, prop.chisq = FALSE)
#Risk of Failure to Appear versus Race (Caucasian or not)
CrossTable(dataLog$appear_Log, dataLog$race_Log, dnn= c("Caucasian", "Risk of Failure to Appear"), prop.t=FALSE, prop.r=FALSE, prop.c=FALSE, prop.chisq = FALSE)
#REQUIRED: graphical contingency tables
#another way of showing race and display text are independent -
#covariance and correlation are close to 0
#race and risk of violence
cov(dataLog$race_Log, dataLog$risk_Log)
cor(dataLog$race_Log, dataLog$risk_Log)
#race and risk of recidivism
cov(dataLog$race_Log, dataLog$recid_Log)
cor(dataLog$race_Log, dataLog$recid_Log)
#race and risk of failure to appear
cov(dataLog$race_Log, dataLog$appear_Log)
cor(dataLog$race_Log, dataLog$appear_Log)
#all are close to 0
#POINT 16: Covariance and correlation
#barplot of various display texts
#they are all perfectly equal! weird!
plot <- ggplot(data = compas, aes(x = factor(DisplayText))) +
geom_bar(stat="count", width=0.7, fill="steelblue") + theme_minimal() +
ggtitle("Display Text vs Frequency") + xlab("Display Text") + ylab("Frequency"); plot
#REQUIRED: barplot
sum(dataLog$raceLog*dataLog$riskLog)/length(dataLog$raceLog)
length(dataLog$raceLog)
sum(dataLog$race_Log*dataLog$risk_Log)/length(dataLog$race_Log)
sum(!dataLog$race_Log*dataLog$risk_Log)/length(dataLog$race_Log)
sum(dataLog$race_Log*dataLog$risk_Log)/length(dataLog$race_Log)
sum(!dataLog$race_Log*dataLog$risk_Log)/length(dataLog$race_Log)
sum(dataLog$race_Log*dataLog$appear_Log)/length(dataLog$race_Log)
#probability of a white person getting a display text of "Risk of Failure to Appear"
sum(!dataLog$race_Log*dataLog$appear_Log)/length(dataLog$race_Log)
sum(!dataLog$race_Log*dataLog$appear_Log)/length(dataLog$race_Log)
sum(dataLog$race_Log*dataLog$appear_Log)/length(dataLog$race_Log)
#probability of a non-white person getting a display text of "Risk of Recidivism"
sum(dataLog$race_Log*dataLog$recid_Log)/length(dataLog$race_Log)
#probability of a white person getting a display text of "Risk of Recidivism"
sum(!dataLog$race_Log*dataLog$recid_Log)/length(dataLog$race_Log)
#probability of a non-white person getting a display text of "Risk of Violence"
sum(dataLog$race_Log*dataLog$risk_Log)/length(dataLog$race_Log)
#probability of a white person getting a display text of "Risk of Violence"
sum(!dataLog$race_Log*dataLog$risk_Log)/length(dataLog$race_Log)
length(which(compas$Ethnic_Code_Text = "Caucasian" & compas$DisplayText = "Risk of Failure to Appear"))
length(which(compas$Ethnic_Code_Text == "Caucasian" && compas$DisplayText == "Risk of Failure to Appear"))
length(which(compas$Ethnic_Code_Text == "Caucasian" & compas$DisplayText == "Risk of Failure to Appear"))
length(which(compas$Ethnic_Code_Text == "Caucasian" & compas$DisplayText == "Risk of Failure to Appear"))/length(compas)
length(which(compas$Ethnic_Code_Text == "Caucasian" & compas$DisplayText == "Risk of Failure to Appear"))/length(compas$Ethnic_Code_Text)
length(which(compas$Ethnic_Code_Text != "Caucasian" & compas$DisplayText == "Risk of Failure to Appear"))/length(compas$Ethnic_Code_Text)
length(which(compas$Ethnic_Code_Text == "Caucasian" & compas$DisplayText == "Risk of Violence"))/length(compas$Ethnic_Code_Text)
length(which(compas$Ethnic_Code_Text != "Caucasian" & compas$DisplayText == "Risk of Violence"))/length(compas$Ethnic_Code_Text)
#probability of a white person getting a display text of "Risk of Failure to Appear"
length(which(compas$Ethnic_Code_Text == "Caucasian" & compas$DisplayText == "Risk of Recidivism"))/length(compas$Ethnic_Code_Text)
#probability of a non-white person getting a display text of "Risk of Failure to Appear"
length(which(compas$Ethnic_Code_Text != "Caucasian" & compas$DisplayText == "Risk of Recidivism"))/length(compas$Ethnic_Code_Text)
#probability of a white person getting a display text of "Risk of Failure to Appear"
p1<- length(which(compas$Ethnic_Code_Text == "Caucasian" & compas$DisplayText == "Risk of Failure to Appear"))/length(compas$Ethnic_Code_Text)
p1
#probability of a non-white person getting a display text of "Risk of Failure to Appear"
p2 <- length(which(compas$Ethnic_Code_Text != "Caucasian" & compas$DisplayText == "Risk of Failure to Appear"))/length(compas$Ethnic_Code_Text)
p2
#probability of a white person getting a display text of "Risk of Violencer"
p3 <- length(which(compas$Ethnic_Code_Text == "Caucasian" & compas$DisplayText == "Risk of Violence"))/length(compas$Ethnic_Code_Text)
p3
#probability of a non-white person getting a display text of "Risk of Failure to Appear"
p4 <- length(which(compas$Ethnic_Code_Text != "Caucasian" & compas$DisplayText == "Risk of Violence"))/length(compas$Ethnic_Code_Text)
p4
#probability of a white person getting a display text of "Risk of Failure to Appear"
p5 <- length(which(compas$Ethnic_Code_Text == "Caucasian" & compas$DisplayText == "Risk of Recidivism"))/length(compas$Ethnic_Code_Text)
p5
#probability of a non-white person getting a display text of "Risk of Failure to Appear"
p6 <- length(which(compas$Ethnic_Code_Text != "Caucasian" & compas$DisplayText == "Risk of Recidivism"))/length(compas$Ethnic_Code_Text)
p6
p1+p2+p3+p4+p5+p6
